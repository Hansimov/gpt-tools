#%%
import os
import json
from pathlib import Path

# python -m pip install --upgrade openai
import openai


def init_os_envs():
    with open(Path(__file__).parent / "secrets.json", "r") as rf:
        secrets = json.load(rf)
    os.environ["OPENAI_API_KEY"] = secrets["openai_api_key"]

    for proxy_env in ["http_proxy", "https_proxy"]:
        os.environ[proxy_env] = secrets["http_proxy"]


init_os_envs()
openai.api_key = os.environ["OPENAI_API_KEY"]


"""
* openai-cookbook: How to format inputs to ChatGPT models
  * https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb

  
# API Request

A chat API call has two required inputs:
1. `model`: The name of the model you want to use
  * Example: "gpt-3.5-turbo", "gpt-4", "gpt-4-0314"
2. `messages`: A list of message objects, where each object has two required fields:
  * `role`: Role of the messenger
    * Choices: "system", "user", "assistant"
  * `content`: Content of the message
    * Example: "Write me a beautiful poem"

Messages can also contain an optional `name` field, which give the messenger a name.
* Example: "example-user", "Alice", "BlackbeardBot".
> Names may not contain spaces.

Typically, a conversation will:
1. start with a `system` message that tells the assistant how to behave
2. followed by alternating `user` and `assistant` messages
> You are not required to follow this format.


# API Response

The response object has a few fields:

1. `id`: ID of the request
2. `object`: Type of object returned
  * Example: `chat.completion`
3. `created`: Timestamp of the request
4. `model`: Full name of the model used to generate the response
5. `usage`: Number of tokens used to generate the replies, counting prompt, completion, and total
6. `choices`: a list of completion objects (only one, unless you set n greater than 1)
  * `message`: Message object generated by the model, with role and content
  * `finish_reason`: Reason for the model stopped generating text (either stop, or length if max_tokens limit was reached)
  * `index`: Index of the completion in the list of choices

# Tips
System messages
* The system message can be used to prime the assistant with different personalities or behaviors.
* Be aware that gpt-3.5-turbo-0301 does not generally pay as much attention to the system message as gpt-4-0314.
* Therefore, for "gpt-3.5-turbo-0301", we recommend PLACING IMPORTANT INSTRUCTIONS IN THE `USER` MESSAGE INSTEAD.
* Some developers have found success in continually moving the system message near the end of the conversation to keep the model's attention from drifting away as conversations get longer.


"""


class Translater:
    def __init__(
        self,
        original_text,
        model="gpt-3.5-turbo",
        task="paper-en",
    ):
        self.model = model
        if task == "paper-en":
            self.system_message = "你是一个学术论文翻译器。你的任务是将给定的中文翻译成英文。你的翻译应当是严谨的和自然的。请勿修改、添加和删除原文中的任何文本和标点。请遵循前述要求，翻译下面这段文字："
        self.original_text = original_text

    def run(self):
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {
                    "role": "user",
                    "content": self.system_message + self.original_text,
                },
            ],
            temperature=0,
        )

        self.translated_text = response["choices"][0]["message"]["content"]
        return self.translated_text


translater = Translater(
    'Recent advances in conditional generative image models have enabled impressive results. On the one hand, text-based conditional models have achieved remarkable generation quality, by leveraging large-scale datasets of image-text pairs. To enable fine-grained controllability, however, text-based models require long prompts, whose details may be ignored by the model. On the other hand, layout-based conditional models have also witnessed significant advances. These models rely on bounding boxes or segmentation maps for precise spatial conditioning in combination with coarse semantic labels. The semantic labels, however, cannot be used to express detailed appearance characteristics. In this paper, we approach fine-grained scene controllability through image collages which allow a rich visual description of the desired scene as well as the appearance and location of the objects therein, without the need of class nor attribute labels. We introduce "mixing and matching scenes" (M&Ms), an approach that consists of an adversarially trained generative image model which is conditioned on appearance features and spatial positions of the different elements in a collage, and integrates these into a coherent image. We train our model on the OpenImages (OI) dataset and evaluate it on collages derived from OI and MS-COCO datasets. Our experiments on the OI dataset show that M&Ms outperforms baselines in terms of fine-grained scene controllability while being very competitive in terms of image quality and sample diversity. On the MS-COCO dataset, we highlight the generalization ability of our model by outperforming DALL-E in terms of the zero-shot FID metric, despite using two magnitudes fewer parameters and data. Collage based generative models have the potential to advance content creation in an efficient and effective way as they are intuitive to use and yield high quality generations.'
)
# 最近，条件生成图像模型的进展取得了令人瞩目的成果。一方面，基于文本的条件模型通过利用大规模的图像-文本对数据集，实现了卓越的生成质量。然而，为了实现细粒度的可控性，基于文本的模型需要长的提示语，而模型可能会忽略其中的细节。另一方面，基于布局的条件模型也取得了显著的进展。这些模型依赖于边界框或分割图，以实现精确的空间条件，结合粗略的语义标签。然而，语义标签无法表达详细的外观特征。本文通过图像拼贴来实现细粒度场景可控性，这允许对所需场景以及其中对象的外观和位置进行丰富的视觉描述，而无需使用类别或属性标签。我们引入了“混合和匹配场景”（M&Ms）的方法，该方法由一个对抗训练的生成图像模型组成，该模型基于拼贴中不同元素的外观特征和空间位置进行条件，并将它们整合成一个连贯的图像。我们在OpenImages（OI）数据集上训练我们的模型，并在从OI和MS-COCO数据集派生的拼贴上进行评估。我们在OI数据集上的实验表明，M&Ms在细粒度场景可控性方面优于基线模型，同时在图像质量和样本多样性方面也非常有竞争力。在MS-COCO数据集上，我们通过使用两个数量级更少的参数和数据，在零样本FID指标方面优于DALL-E，突显了我们模型的泛化能力。基于拼贴的生成模型具有提高内容创作效率和效果的潜力，因为它们易于使用且产生高质量的生成物。

if __name__ == "__main__":
    translater.run()
    print(translater.translated_text)
