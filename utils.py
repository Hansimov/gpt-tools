#%%
import os
import json
from pathlib import Path

# python -m pip install --upgrade openai
import openai


def init_os_envs():
    with open(Path(__file__).parent / "secrets.json", "r") as rf:
        secrets = json.load(rf)
    os.environ["OPENAI_API_KEY"] = secrets["openai_api_key"]

    for proxy_env in ["http_proxy", "https_proxy"]:
        os.environ[proxy_env] = secrets["http_proxy"]


init_os_envs()
openai.api_key = os.environ["OPENAI_API_KEY"]


"""
* openai-cookbook: How to format inputs to ChatGPT models
  * https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb

  
# API Request

A chat API call has two required inputs:
1. `model`: The name of the model you want to use
  * Example: "gpt-3.5-turbo", "gpt-4", "gpt-4-0314"
2. `messages`: A list of message objects, where each object has two required fields:
  * `role`: Role of the messenger
    * Choices: "system", "user", "assistant"
  * `content`: Content of the message
    * Example: "Write me a beautiful poem"

Messages can also contain an optional `name` field, which give the messenger a name.
* Example: "example-user", "Alice", "BlackbeardBot".
> Names may not contain spaces.

Typically, a conversation will:
1. start with a `system` message that tells the assistant how to behave
2. followed by alternating `user` and `assistant` messages
> You are not required to follow this format.


# API Response

The response object has a few fields:

1. `id`: ID of the request
2. `object`: Type of object returned
  * Example: `chat.completion`
3. `created`: Timestamp of the request
4. `model`: Full name of the model used to generate the response
5. `usage`: Number of tokens used to generate the replies, counting prompt, completion, and total
6. `choices`: a list of completion objects (only one, unless you set n greater than 1)
  * `message`: Message object generated by the model, with role and content
  * `finish_reason`: Reason for the model stopped generating text (either stop, or length if max_tokens limit was reached)
  * `index`: Index of the completion in the list of choices

# Tips
System messages
* The system message can be used to prime the assistant with different personalities or behaviors.
* Be aware that gpt-3.5-turbo-0301 does not generally pay as much attention to the system message as gpt-4-0314.
* Therefore, for "gpt-3.5-turbo-0301", we recommend PLACING IMPORTANT INSTRUCTIONS IN THE `USER` MESSAGE INSTEAD.
* Some developers have found success in continually moving the system message near the end of the conversation to keep the model's attention from drifting away as conversations get longer.


"""


class Translater:
    def __init__(self) -> None:
        self.model = "gpt-3.5-turbo"
        self.system_message = "你是一个学术论文翻译官。你的任务是将英文翻译成中文。你的翻译结果应当是专业的、流畅的、严谨的、精确的。"

    def run(self):
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": self.system_message,
                },
                {
                    "role": "user",
                    "content": 'Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses\' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". For GPT-4, responses to 13 questions were concordant, 15 discordant, and 3 were unable to be assessed. There were 35 responses with no majority. Responses from both LLMs were largely devoid of overt harm, but less than 20% of the responses agreed with an answer from an informatics consultation service, responses contained hallucinated references, and physicians were divided on what constitutes harm. These results suggest that while general purpose LLMs are able to provide safe and credible responses, they often do not meet the specific information need of a given question. A definitive evaluation of the usefulness of LLMs in healthcare settings will likely require additional research on prompt engineering, calibration, and custom-tailoring of general purpose models.',
                },
            ],
            temperature=0,
        )
        print(
            response["choices"][0]["message"]["content"],
        )


translater = Translater()
translater.run()
